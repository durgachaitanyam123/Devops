---> About Kernel:
The Linux® kernel is the main component of a Linux operating system (OS) and is the core interface between a computer’s hardware and its processes. It communicates between the 2, managing resources as efficiently as possible.

The kernel is so named because—like a seed inside a hard shell—it exists within the OS and controls all the major functions of the hardware, whether it’s a phone, laptop, server, or any other kind of computer.

What the kernel does
The kernel has 4 jobs:

Memory management: Keep track of how much memory is used to store what, and where

Process management: Determine which processes can use the central processing unit (CPU), when, and for how long

Device drivers: Act as mediator/interpreter between the hardware and processes

System calls and security: Receive requests for service from the processes
The kernel, if implemented properly, is invisible to the user, working in its own little world known as kernel space, where it allocates memory and keeps track of where everything is stored. What the user sees—like web browsers and files—are known as the user space. These applications interact with the kernel through a system call interface (SCI).

---> Linux Directories:

/etc: The /etc directory contains system configuration files for all the services, scripts, and third-party applications that are installed. This directory is considered the nerve center of the Linux system.

/home: The /home directory is a directory that contains a user’s personal folders and files. On a graphical Linux system, the home directory, by default, contains folders such as Desktop, Documents, Downloads, Pictures, Videos, and Public.

In addition, the /home directory contains personal configuration files which are prefixed with a dot (.). These are hidden files that contain user-specific settings for the login shell session.

/var: The /var directory stores system-generated variable files, which include log files, caches, and spool files just to mention a few.

/usr: The /usr directory ranks as one of the most important directories due to the enormous amount of data it holds. The directory contains system-wide read-only files. These include libraries, user binaries and their documentation, programs, and system utilities.

/tmp: The /tmp directory is a directory that stores temporary files and many programs use this directory to create lock files and keep the temporary storage of data. Do not delete files under the/tmp directory unless you know exactly what you are doing! Many of these files are critical for presently running programs and removing them may affect a system crash.

/bin: The /bin directory contains user binaries, executable programs, and common system commands that are used by all users in the system. These include ls, pwd, cat, mkdir, cd, mv, cp, du, df, tar, rpm, wc, history, etc.

/opt: The /opt directory is a directory that is reserved for add-on packages and third-party software applications that are not included by default in the system’s official repositories.
For example, when you install applications such as Skype, Discord, Spotify, and Java, to mention a few, they get stored in the /opt directory.

---> What is Shell?
A Shell is basically a command-line interpreter between user and kernel or a
complete environment specially designed to run commands, shell scripts, and
programs. In this, whenever a user enters human-readable commands (input
commands) through the keyboard, the shell communicates with the kernel to
execute these commands, and display output in a shell script. Just as there are
different flavors of operating systems, there are also different types of shells.


Linux Commands:     {Command FlagName FileName}
1. whoami
2. pwd
3. cd ..
4. cat
5. clear
5. sudo -i
6. su username {Switch User}
7. useradd username
8. passwd username
9. userdel username
10. groupadd groupname
11. usermod -a -G groupname username
12. groupdel groupname
13. mkdir Dir_Name
14. rmdir Dir_Name
15. touch filename
16. rm filename
17. ls
18. ls -l or ll {Longlist}
18. ls -a
19. vi editor(you can write something in File)
20. id username/groupname
21. cp filename /pathname
22. mv filename /pathname {To change file name or moving the file path}
23. hostnamectl set-hostname hostname
24. cat /etc/os-release
25. cat /etc/passwd
26. cat /etc/group
27. man command_name {To Know Brief About The particular command}
28. vi /etc/sudoers {How to give Sudoers file permissions to users}
29. chmod permissions filename
30. chown username filename
31. stat filename : this cmd prints the more info about the file
32. lsof -u username
33. free -m {Memory}
34. df -h {Filesystem}
35. history
36. [root@devops /]# getent group AB
37. vi /etc/ssh/sshd_config {To Make Configurations}
38. systemctl restart sshd { For Restarting File to updtae the changes}
39. rpm -qa {To cehck the number of applications installed in your system}
40. cmp  {Allows you to check if two files are identical}
41. usermod {change existing users data}
42. whatis {Find what a command is used for}
43. diff {Find the difference between two files}
44. alias {Create custom shortcuts for your regularly used commands}
45.  less  {Linux command to display paged outputs in the terminal}
46. cat > filename {To add some data into the file}
47. ifconfig
48. netstat

AB:x:1004:A1,A2,A3,A4
Topic vi editor Insert Mode:
i - Start Writing from one step from the current position
a - Start Writing from the current position
I - Start Writing from beginning 
o - insert new line below the current line
O - insert new line above the current line

ESC Mode
G - To go to last line
gg - go to first line
 5gg - go to 5th line
yy - To copy the currenr line
  5yy - copy 5lines from the current line
p - Paste the copied line below the current line
   5p - paste the copied content  times below the current line
dd - delete the current line
   5dd - delete the 5 lines from current line
ctrl + u - undo the previous action
ctrl + r - redo the previous action
:se nu
:q! - Quit
:w - To save the changes
:wq! - To save and quit from vi editor
  


Task: 1.Command to print the output of users in particular group?
      2.Users in particular Group should have sudoers permission? {Dont give sudoers permission to users}
      3. Adding user and giving paaswd to him. Now, is it possible for him to access the server from another system.
2things to remember: 1.To make configuration
                     2. To restart the confiuration file
      
       4. How to zip and un-zip tar files in linux
          tar -cvf filename foldername
          tar -xvf tar-file
       5. Is it possible to concatenate two commands or Is it possible to give 2 commands at a time?

          How to limit the sudoers permission to users

Today's Discussion:

ps
kill
YUM(YellowDog Updater MOdified)  
curl and wget
rpm -qa (If i want to see particular app use command rpm -qa | grep appname
head command
tail command
sed command is used for Modifying the data in the file { sed 's/olddata/newdata' filename }
grep command {grep -c/-n/-w "string" filename}
sort command {sort -r/-n filename}
find command {find / -type d -name abc} {find /home -iname homw.txt} {find / -perm /u=r}
wc command(word count) it prints number of lines, words and character in a file {wc filename} {-c filename} -c: bytes

##################################################################################################

TASK: Print Filename Which are created since 2Days
      Print Filename Which are more than 10mb and less than 15mb
      print Files with amar group?
      How to print the other lines excluding first 2 lines {use head command} ps -ef | head -n -2
      How many duplicates arein file?
      Which line has the duplicate data?
       
      
      


Today's Topic:

Text Processing Tools:

0. grep: grep -i -w stringname filename {it removes the duplicates}
         grep -i ^stringname filename {it prints lefthand first words}
         ps -ef | grep filename(java)

1. find: find  / -name os-release
         find  / -name passwd
         find  / -name *.txt
         find  / -size +10M
         find  / -atime +2
         find  / -ctime +2 (2daysbefore)
2. echo: It will Print Messages which you given
3. tr:   The tr command is a UNIX command-line utility for translating or deleting characters.
         cat greekfile | tr [a-z] [A-Z]
         echo "{NEW one}" | tr "{}" "()"
         echo "Welcome    To    INDIA" | tr -s " "
         echo "Sorry to say this, You are not eligible" | tr -d "not"
4. sort: SORT command is used to sort a file, arranging the records in a particular order.
         sort filename
         sort -r filename {reverese format}
         sort -n filename {for numerical data}
         sort -nr filename
         sort f2>f3
         sort -o f2 f3
         cat f3
5. uniq: The uniq command in Linux is a command-line utility that reports or filters out the repeated lines in a file.
         uniq -c f2
         uniq -u f2 (it prints only unique lines)
         uniq -d f2 (it prints only repeated lines)
6. awk:  extract the fields/columns based on some deliminator/separator from the output
          free -m | grep Mem | awk -F " " '{print $7}'
          uname -a|awk -F " " '{print $3}'|awk -F '.' '{print $5}'
         
7. uname -a: To know the kernel version
8. shell: it is like interpreter. Means it takes input and send it to the kernel. Green color cursor is the representer.
               COMMAND:     cat /etc/shells
9. cut: cut command is also like awk command but small differences are there.
 df -m /|tail -n1|cut -d ' ' -f1 

################################################################################
Today's Topic Shell Script:

---> What is Shell Scripting
 Text files containing commands to execute by a shell are called shell scripts. In this,
long and repetitive series of commands are compiled into a single script that can be
stored and executed at any time, thereby reducing programming efforts. In shell
scripts, repetitive work is largely avoided.

---> Shell scripts can be written for a variety of reasons:
* Keeping repetitive tasks to a minimum.
* Can be used by system administrators for routine backups.
* Monitoring the system.
* Adding new functions to the shell.
* Shell scripting allows you to create your own tools.
* System admin can automate daily tasks.

---> What do you mean by Shell variable?
Shell variables are integral parts of all Shell programs and scripts. In general, we
know that variables usually store data either in the form of characters or numbers.
Shell also stores and manipulates information using variables in its programs.
Generally, shell variables are stored as strings. Variables in the shell provide the
information needed for scripts/commands to execute. In the following example, a

shell variable is created and then printed:
variable ="Hello"
echo $variable

---> What are different types of variables mostly used in shell scripting?
Shell scripts usually have two types of variables:

System-defined variables: Also called environment variables, these are special
built-in variables in the Linux kernel for each shell. They are normally defined in
capital letters by the OS (Linux) and are standard variables.
Example: 
SHELL

It is a Unix Defined or System Variable, which specifies the default working shell.
User-defined variables: These variables are created and defined by users in
order to store, access, read, and manipulate data. In general, they are defined in
lowercase letters. The Echo command allows you to view them.
Example:   
$ a=10
In this case, the user has defined the variable ‘a’ and assigned it the value 10.


*. shell: it is like interpreter. Means it takes input and send it to the kernel. Green color cursor is the representer.
               COMMAND:     cat /etc/shells

0. cut: cut command is also like awk command but small differences are there.
 df -m /|tail -n1|cut -d ' ' -f1 

1. create simple_script.sh

2. vi simple_script.sh

#!/bin/bash
echo "This is My First Script Team"
:wq!

3. cat simple_script.sh

4. ls -l simple_script.sh

5. chmod +x simple_script.sh

6. ./simple_script.sh or /simple_script.sh or sh simple_script.sh

7. Suppose you are in some other location and you want to run shell script.
   there you need to give path of s
RUN TIME VARIABLE:

#!/bin/bash
echo "Please provide value for a"
read a
echo "Please provide value for b"
read b
expr $a + $b

POSITIONAL PARAMETER

#!/bin/bash
a=$1
b=$2
c=$3
 expr $a + $b - $c
 expr $a - $b + $c
 expr $a / $b \* $c

To Print The Output We need to give Variable Values of a & b
./sample.sh 10 10

To Print The Output in Debug Mode command will be like 
sh -x sample.sh 10 10

LOCAL VARIABLE:
  #!/bin/bash
  month=jan
  touch fileA_$month                                                     a     b     c  ""

OUTPUT VARIABLE:
    
   memory=`free -m | grep Mem:|awk -F ' ' '{print $7}'`
   kernel=`uname -a|awk -F " " '{print $3}'`
   os=`cat /etc/os-release | grep ^NAME|awk -F '"' '{print $2}' | awk '{print $1}'`
   storage=`df -m /| grep -v Filesystem | awk '{print $4}'`
   
   
   echo "My Available MEMORY is $memory" 
   echo "My KERNEL_VERSION is $kernel"
   echo "My OS is $os"
   echo "MY STORAGE   is $storage                            

TOPIC:

Environment Variables: 
user level: Particular user Variables
global level: Entire system(any user can use them)

xargs: it prints the number of lines in a single line.
ifconfig eth0 |head -n2|xargs|awk '{print$1},"device ip address is",$6

>SEARCH BASH MAN PAGE
>IN THIS PAGE FIND CONDITIONAL EXPRESSIONS
>https://www.man7.org/linux/man-pages/man1/bash.1.html
--------------------------------------------------------------------------------------------------------------------------
#!/bin/bash
a=2
b=7
if [ $a -ge $b ]
then
  echo "The variable 'a' is greater than the variable 'b'."
else
  echo "The variable 'b' is greater than the variable 'a'."
fi
-----------------------------------------------------------------------------------------------------------------------------
if [ -d $1] 
then
 echo "$1 is a directory and it exists
else
 echo "$1 is not a directory and not exists"

fi

chmod +x filename.sh

./filename.sh /etc ---- It shows Whether there is a Directory or not

###############################################################################################################################
19/12/2022

More Examples on if Statements

#!/bin/bash

FILE=/RedSandal/Pushpa
if test -f "$FILE"; then
    echo "$FILE exists."
else
        echo "There is no such File, Creating New File..."
        mkdir /Aim
        touch Shoot
        mv Shoot /Aim
        echo "new file created and moved to /Aim Directory "
fi


####################################################################################################################################

(OR STATEMENTS)

#!/bin/bash
os=`cat /etc/os-release |grep ^NAME|awk -F '"' '{print $2}'|awk '{print $1}'` = Ubuntu

if [ $os = Amazon ] ||  [ $os = Redhat ] || [ $os = Centos ]

then

    echo "This is RedHat Family, so proceed with yum."

else

   echo "This is Not RedHat Family, So can't proceed further"

fi

#Whenever if you are using commands in shell script you should use `back quote`
########################################################################################
(&& STATEMENTS)

#!/bin/bash
os=`cat /etc/os-release |grep ^NAME|awk -F '"' '{print $2}'|awk '{print $1}'
kernel=`uname -a|awk -F " " '{print $3}'|cut -b -4` 

if [ $os = Amazon ] && [ $kernel -ge 5.11]

then
    echo "Your System is eligible for proceeding further"

else
    echo "Your system is not eligible for next step, Please EXIT."

fi

#######################################################################################################################
#!/bin/bash
user_id=`cat /etc/passwd|grep -i -w ^ec2-user|awk -F ':' '{print $3}'
echo "$1 user id is $user_id"

#########################################################################################################################
#PRINTING USER ID
#!/bin/bash
#START
User_Id=`id $1 | awk '{print $1}'`

echo " $1 userid is $User_Id"
##############################################################################################################################3
(elseif STATEMENTS)

#!/bin/bash

os=`cat etc/os-release |grep ^NAME|awk -F '"' '{print $2}|awk '{print $1}'`

#If 1st condition satisfies means it prints echo otherwise it will move to next
condition and checks wheteher it is satisfying or not.

if [ $os = Amazon]
then
  echo "This is Amazon"

elif [ $os = redhat ]
then
   echo "This is Redhat"

elif [ $os = centos]
then
   echo "This is centos"

fi
##########################################################################################
(for Loop STATEMENT)

#!/bin/bash
for username in `echo "$*"`
do
    user_id=`cat /etc/passwd| grep -i -w ^$1|awk -F ':' '{print $3}'`

       echo " $1 userid is $user_id"
done

#add username in the place of $1
###############################################################################################

#!/bin/sh
echo "Script Name: $0"
echo "First Parameter of the script is $1"
echo "The second Parameter is $2"
echo "The complete list of arguments is $@"
echo "Total Number of Parameters: $#"
echo "The process ID is $$"
echo "Exit code for the script: $?"

./ss.sh learning command line arguments

#################################################################################################

TODAY's TOPIC

#!/bin/bash
#START

set `date`
echo "day is $1"
echo "month is $2"
echo "date is $3"
echo "Time is H:M:S $4"
echo "Time Zone is $5"
echo "year is $6"

set -x

#END

####################################################

[root@devops /]# cat grp.sh
#!/bin/bash
#This is Task1
echo "1.Group AB"
echo "2.Group CD"
echo "Select Any one Option Above"
read -r ch
case $ch in
        1)echo "GROUP ID: `cat /etc/group|grep ^AB| awk -F ':' '{print $3}'` GROUP USERS: `cat /etc/group|grep ^AB| awk -F ':' '{print $4 $5 $6}'`" ;;
        2)echo "GROUP ID: `cat /etc/group|grep ^CD| awk -F ':' '{print $3}'` GROUP USERS: `cat /etc/group|grep ^CD| awk -F ':' '{print $4 $5 $6}'`" ;;
        *)echo "Invalid Option"

        esac
[root@devops /]# sh grp.sh
######################################################
#!/bin/bash
#This is Task2

opt=y
while [ $opt = y -o $opt = Y ]
do
        echo  "Please Enter the number: "
        read -r number 
        if [ $number -le 51 ]; then
                sq=`expr $number \* $number`       
                echo "square of provided number $number: $sq"
        else
                echo "number is not in given range"
        fi
        echo "Do you want to continue [Yes/No]: "
        read -r wish
        if [ $wish = yes -o  $wish = YES ]; then
                continue
        else
                echo "Thanks For Exiting.."
                exit
        fi
done
#############################################################
#!/bin/bash
for ((i=15; i>10; i++))
do
        echo $i
done
#################################################################################################
#!/bin/bash
#To Check Wheter The Application is Installed OR Not
#START
if ! [ -x "$(command -v jenkins)" ]; then
  echo 'Error: jenkins is not installed.' >&2
  else
          echo "jenkins is installed"
fi
######################################################################################
#!/bin/bash
#To Check Wheter The Application is Installed OR Not
#START
if which git >/dev/null; then
    echo exists
else
    echo does not exist
fi
##########################################################################################
#!/bin/bash
#To Check Wheter The Application is Installed OR Not
#START
if which httpd >/dev/null; then
    echo exists{Application is already installed}
else
    echo "App is not installed hence installing..."
    sudo yum install httpd -y
    sudo systemctl enable httpd
    sudo systemctl start httpd
   a=`sudo systemctl status httpd`
    echo "Application Status is $a"
fi
###############################################################################################
top command:

us: user cpu time (or) % CPU time spent in user space
sy: system cpu time (or) % CPU time spent in kernel space
ni: user nice cpu time (or) % CPU time spent on low priority processes
id: idle cpu time (or) % CPU time spent idle
wa: io wait cpu time (or) % CPU time spent in wait (on disk)
hi: hardware irq (or) % CPU time spent servicing/handling hardware interrupts
si: software irq (or) % CPU time spent servicing/handling software interrupts
st: steal time - - % CPU time in involuntary wait by virtual cpu while hypervisor is servicing another processor (or) % CPU time stolen from a virtual machine

Every program or task that runs on a computer system occupies a certain amount of processing time on the CPU. If the CPU has completed all tasks it is idle.

##############################################################################################

CPU performance is one aspect of measuring the performance of a system, which is essential to measure the overall system-performance.

When a Linux system CPU is occupied by multiple processes, it is not available to process other requests, and the remaining pending requests must wait until the CPU is free.

If your system is under stress, it will slow down your application and becomes a bottleneck in the system.

There are numerous tools available to monitor and display CPU performance in Linux such as top, htop, glances, etc.

In this tutorial we have added two shell scripts to monitor the CPU utilization on Linux system, which is very useful when user has only few systems to monitor.

These scripts will trigger an email to the corresponding email id when the system reaches a given threshold.

#!/bin/bash
# This script monitors CPU and memory usage

while :
do 
  # Get the current usage of CPU and memory
  cpuUsage=$(top -bn1 | awk '/Cpu/ { print $2}')
  memUsage=$(free -m | awk '/Mem/{print $3}')

  # Print the usage
  echo "CPU Usage: $cpuUsage%"
  echo "Memory Usage: $memUsage MB"
 
  # Sleep for 1 second
  sleep 1
done
______________________________________________________________________________________________________________________________________
#vi /opt/scripts/cpu-alert.sh

#!/bin/bash
cpuuse=$(cat /proc/loadavg | awk '{print $3}'|cut -f 1 -d ".")
if [ "$cpuuse" -ge 90 ]; then
SUBJECT="ATTENTION: CPU load is high on $(hostname) at $(date)"
MESSAGE="/tmp/Mail.out"
TO="gowraajay@gmail.com"
  echo "CPU current usage is: $cpuuse%" >> $MESSAGE
  echo "" >> $MESSAGE
  echo "+------------------------------------------------------------------+" >> $MESSAGE
  echo "Top 20 processes which consuming high CPU" >> $MESSAGE
  echo "+------------------------------------------------------------------+" >> $MESSAGE
  echo "$(top -bn1 | head -20)" >> $MESSAGE
  echo "" >> $MESSAGE
  echo "+------------------------------------------------------------------+" >> $MESSAGE
  echo "Top 10 Processes which consuming high CPU using the ps command" >> $MESSAGE
  echo "+------------------------------------------------------------------+" >> $MESSAGE
  echo "$(ps -eo pcpu,pid,user,args | sort -k 1 -r | head -10)" >> $MESSAGE
  mail -s "$SUBJECT" "$TO" < $MESSAGE
  rm /tmp/Mail.out
else
echo "Server CPU usage is in under threshold"
  fi

cat /proc/loadavg : 0.02 0.02 0.00 1/169 3979
 The first three columns measure CPU and IO utilization of the last one, five, and 10 minute periods. The fourth column shows the number of currently running processes and the total number of processes. The last column displays the last process ID used.

https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/4/html/reference_guide/s2-proc-locks

########################################################################################
----FOR LOOP

a=`expr$a+1`
a=2

a=0
# -lt is less than operator

#Iterate the loop until a less than 10
while [ $a -lt 10 ]
do
	# Print the values
	echo $a
	
	# increment the value
	a=`expr $a + 1`
done

########################################################################################
#!/bin/bash
COLORS=" RED BLUE GREEN YELLOW BLACK "
#iterate the for loop until total values printed

for COLOR in $COLORS
do
    echo "COLOR: $COLOR"
done

########################################################################################
i=1
for day in mon tue wed thu fri sat sun
do
echo -n "Day $((i++)) : $day"    = day1 : mon         day2:tue
if  [ $i -eq 7 -o $i -eq 8 ];
then
 echo "$day is WEEKEND"
 
 continue;
 fi
  echo "weekdays"
  done
#########################################################################################
How to Create and Manage Cron Jobs on Linux
Cron is one of Linux’s most useful tools and a developer favorite because it allows you to run automated commands at specific periods, dates, and intervals using both general-purpose and task-specific scripts. Given that description, you can imagine how system admins use it to automate backup tasks, directory cleaning, notifications, etc.

Cron jobs run in the background and constantly check the /etc/crontab file, and the /etc/cron.*/ and /var/spool/cron/ directories. The cron files are not supposed to be edited directly and each user has a unique crontab.

$ crontab -e

#Install crontab
yum install cronie

Cron Syntax:

A B C D E USERNAME /path/to/command arg1 arg2
OR
A B C D E USERNAME /root/backup.sh

15 03 22 * * root /root/backup.sh

Explanation of above cron syntax:

A: Minutes range: 0 – 59
B: Hours range: 0 – 23
C: Days range: 0 – 31
D: Months range: 0 – 12
E: Days of the week range: 0 – 7. Starting from Monday, 0 or 7 represents Sunday
USERNAME: replace this with your username
/path/to/command – The name of the script or command you want to schedule

that’s not all. Cron uses 3 operator symbols which allow you to specify multiple values in a field:

Asterisk (*): specifies all possible values for a field
The comma (,): specifies a list of values
Dash (-): specifies a range of values
Separator (/): specifies a step value

Cron Job Examples
The first step to running cron commands is installing your crontab with the command:
# crontab -e

Run script.sh at 3 am every day:
30 16 2 * * /path/to/script.sh

Run /scripts/phpscript.php at 10 pm during the week:
0 22 * * 1-5 /scripts/phpscript.php

List cron jobs.
# crontab -l
OR
# crontab -u username -l

Delete all crontab jobs.
# crontab -r

Delete Cron job for a specific user.
# crontab -r -u username

How to see Users crontab
# crontab -u username -l
##############################################################################################
https://www.javatpoint.com/while-loop-shell-scripting

#############################################################################

How to connect to Server To Server

let me try like this:
There are 2 Servers called ServerA & ServerB
ifconfig command in Server B
#ssh root@privateip in ServerA -- YOU WILL FAIL

By default Passwordless authentication is disabled, we should enable the password.
IN SERVER DO BELOW:
#vi /etc/ssh/sshd_config
#systemctl restart sshd
CREATE PASSWD TO ROOT USER
Firstly login as root user & give passwd
Now, Do the same like as we did in firsttime
#ssh root@privateip in ServerA
now it will ask passwd & give passwd

2nd WAY..
--now, ServerA
#ssh-keygen
#cd /root/.ssh
#ls
#cat id_rsa.pub {copy the key here}
--now, ServerB
#cd /root/.ssh/
#ls
#vi authorized_keys {paste the key here}

finally, do ssh privateip of ServerB

##################################################################################################################################
                                                         AWS
##################################################################################################################################
IAAS: Infrastructure as a service.
PAAS: Platform As A Service
SAAS: Software As A Service 

How To Launch AWS INSTANCE::
1)CHOOSE AN AMI-- Amazon Machine Image
2)Instance Types-- a)General Purpose b)CPU Optimize c)Storage Optimize D)Memory Optimize
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html
3)Configure Instance Details-- a)Network b)Instance Count c)Iam d)Advanced Settings e)storage f)tags g)security groups

###########################################################################
IP ADDRESS:
1)STATIC
2)DYNAMIC
3)PRIVATE  eg: 17.10.0.0
4)PUBLIC   eg: 255.254.254.254/0
How to Calculate IP ADDRESSES:
192.124.11.01

calculate IP Address:

####################################################################################
IAM USERS,POLICIES,ROLES-------
CUSTOM POLICIES----
Suppose i don't want to give S3 Bucket Full Access. I want to give permissions only to particular S3 Bucket only. Here, CUSTOM POLICY comes into picture.
---select create "custom policy"
---select visual editor (or) Json
---select service eg: S3
---select Actions
---Access Level {Select any of those}
---Review Policy {Give the name and desp}

@Managed Policy Created
---select "Inline policy" {That policy is created to particular user only and it is not added to policies}
---select visual editor or Json
---select Service eg:S3
---select Actions {eg: List Bucket}
---Resources Will get enable here
---Add arn {give bucket name} (arn:aws:s3:::bucket_name)
---Review Policy
@Inline Policy Created

PERMISSION BOUNDARIES----
Suppose Particular user is having AdmicFullAccess and you want to restrict or set a boundary for him in using Admin_Full_Access.
---Permission Policies {It shows how many permissions are addded to a user Like i.e.,Authorization/Access}
---Permission Boundaries
---select Set Boundary
---select policy {s3 bucket}
###############################################################################################################################

ELASTIC BLOCK STORAGE(EBS)----
Suppose You are not having enough storage and you need some more. what will you do, just simply add EBS and mount it to your server.
---Create New Volume with desired Size value in EBS
---Make sure the EBS Volume and instances are in the same zone
---Select the created volume, right click & select "attach volume" option
---Select the ec2 Instances in the instance box
---now login to your instnace and list available disks using the command "lsblk"
---check if the volume has any data using the "sudo file -s /dev/xvdf"  {/dev/xvdf: data}
---format the volume to the "ext4" filesystem using the "sudo mkfs -t ext4 /dev/xvdf or sudo mkfs -t xfs /dev/xvdf"
---now create a directory of your choice to mount our new ext4 volume. "sudo mkdir /Directory_Name"
---Mount the volume to Directory_Name using "sudo mount /dev/xvdf /Directory_Name"
---Do, df -h
###################################################################################################################################

ELASTIC FILE SYSTEM(EFS)----
one Petabyte is equal to 1,000 Terabytes
---Search EFS in Search Bar
---Select create EFS
---You can attach EFS to Ec2-Instance, Hit Attach Button
---You can attach it in two ways
--- 1.Mount via DNS 2.Mount via IP
---https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-mount-helper-ec2-linux.html

---> How do I control which Amazon EC2 instances can access my file system?

You control which EC2 instances can access your file system using VPC security group rules and IAM policies. Use VPC security groups to control the network traffic to and from your file system. Attach an IAM policy to your file system to control which clients can mount your file system and with what permissions, and use EFS Access Points to manage application access. Control access to files and directories with POSIX-compliant user and group-level permissions.
####################################################################################################################################

SIMPLE STORAGE SERVICE(S3)----
Its all started on 2004. It is first started with Simple Use Service(SUS). S3 released on March 2006 got popularity.
1.Scalability: You can store huge data upto PetaBytes starting with 0.001bytes. You Need to pay only for the storage that you have used. 
2.Availability: Suppose you have pendrive and in that drive you have one copy of particular data and you loosed data due to pendrive corrupted or virus, you can't get it back.

 Coming to s3 It Stores your data in three copies Bcoz you have issue with data center.
3.Internet: Only internet is enough to retrieve your data whenever you required

---Search S3 in Search Bar
---Create Bucket (You need Provide Globally Unique name to your bucket, it won't accept duplicate names)
---Select AWS region (ap-south-1)
---Create Bucket
@Bucket_created
---You can Upload the files Using "UPLOAD" Tab
---The Files that are stored in Bucket are called "OBJECTS"
---You can Download The files using "DOWNLOAD" Tab
---You can Create Folders in Bucket 
---You can give access on your files to other users publicly by generating URL
---There are 2 Types of Websites 1.DYNAMIC WEBISTE 2.STATIC WEBSITE
---Select any Object 
---By default the URL is in private mode and you can't access the url
---Select "Object Actions" and select "Make Public"
---Before Make it as Public, Make some configurations in Bucket Permissions.
---You will Find BLOCK_PUBLIC_ACCESS, Edit it
---Save Changes
@NOW_DATA_IS_PUBLICLY_ACCESSED

S3BUCKET_STORAGE_CLASSES----
---Take a Particular Region #ap-south-1
---Using S3 Bucket, You can store the data in AZ1
---Due to Un-Predictable Conditions like short circuit, earthquakes. You lost the data in that AZ
---You will raise a request to AWS for your data. It will check in AZs now 
---Storage Classes: 1)Standard Storage 2)Reduced Redundant Storage 3) Intelligent Tier 4)Standard Storage- IA 5) One Zone-IA 6)Glacier 7)Glacier Deep 
--- https://aws.amazon.com/s3/storage-classes/
---Four Parts 1)Frequently Accessed 2)In-frequently 3)Archival
---During Files upload you can give the Access Control List Permissions, Now you can choose Grant Public Read Access
---If you select the Properties. There you can select the storage classes. By Default Standard calss is selected.

CREATING_BUCKET_POLICY----
Bucket Policy is to giving permissions to each and every file in the bucket for public access
---Go To Permissions
---Bucket Policy
---Select Edit
---Select Policy Generator
---Principal {If you want to give Permission to all users give *}
---Actions {If you want to give All actions SELECT ALL ACTIONS}
---Bucket ARN {Copy the bucket ARN & Paste it over Here}
---Click On Generate Policy
---You will get Json Format and copy and paste it in S3 Console
---At Resource add /* to apply it for all files for public Access, Give Inside the " qutotation.

(To Inspect The Page. Right Click On your Mouse Select Inspect)
 
ACCESSING_PRIVATE_OBJECT_PRESIGNED_URL----
---Delete The Bucket Policy
---Login to your server and give Public Access Key and secret key
---Go to your name on top most right hand corner, Click on that
---Select security Credntials
---Generate Credentials at access keys
---Go to your instance and give "aws configure" command
---Paste the access keys and Secret Keys & also give the Region Name {AKIAYHTJR3DZ6H4FOEHK       L+tEGTLH5ZAXEY4fXE8DNyyy5UicscyI7tHpzW0e}
---Search AWS S3 CLI COMMANDS in Browser and search for Presign Command
---Now, in CLI give aws s3 presign and S3 URI
---Give Expiration Time using --expires-in (eg. 60secs)
---It will generate URL and u can try in web.

S3_BUCKET_ENCRYPTION----
---The Files that are stored in S3 Bucket are not encrytpted by default
---If you want to Encrypt the files. Just do the following 
--- Go to properties and make changes
---Static Website is disabed, Enable Itsss
---Enable The encryption as well

Copying files from EC2 to S3 is called Uploading the file

# To List the S3 Bucket 
aws s3 ls s3://<S3bucketName>

# To copy the files from EC2 to S3
aws s3 cp <Fully Qualified Local filename> s3://<S3BucketName>

Copying files from S3 to EC2 is called Downloading the files

# To copy the files from S3 to EC2
aws s3 cp s3://<S3BucketName> <Fully Qualified Local filename/Directory>
#####################################################################################################################################

SNAPSHOT----
We are attaching EBS Volume to EC2 Instances and we have data in EBS Volume. Suppose if we lost the data in EBS Volume, we can create Snapshot for Backup of Data. The Data that is stored in EBS Volume will be copied to snapshot. The Snapshots are stored in S3 Buckets in Backend. The Aws will hide that buckets for security purposes. They Will Call it as "OPAQUE" S3 Buckets. 
E.g: If Your Server is having attached to 10 Number of volumes and you need to create snapshot each and every volume. You can't create Snapshots at a time.

Charges: Only Charged to Snapshots

Amazon Machine Image(AMI)----
Actually What is AMI, the system configurations along with the applications installed on that server. You can create Image to you server. It is like backup to your server. For instance your server is crashed due to overload issues. Here with the help of AMI you can backup your server. You are generally creating servers with custom AMIs.

  AMI will create a template. What Template will do it will stores the OS, Installed applications with versions, Hosted Servers, Settings and also create a snapshot for all the volumes. AMI Means Template + Snapshot of EBS volumes. Now, You don't require extra configuration and Maintainence.

Charges: Only Charged to Snapshots and No charges to AMI Templates.
Costing: Charges are levies only on used storage of snapshot.
For eg: You have taken 100Gib of EBS but you have used only 50Gib only. You will get charges of 50Gib.
---Snapshots should be always lessthan or equal to EBS volumes.

---@@@CREATING SNAPSHOTS
---Go To Elastic Block Storage
---Select Volumes
---Select Volume that you want to attach SNAPSHOT
---Click on Actions
---Click on Create Snapshot
---Here, Give Description and Tags
@@@SNAPSHOT CREATED.

PORT NUMBERS----
---A port number is a way to identify a specific process to which an internet or other network message is to be forwarded when it arrives at a server. All network-connected devices come equipped with standardized ports that have an assigned number.

############################################################################################################################
https://www.cloudflare.com/learning/network-layer/what-is-a-computer-port/
############################################################################################################################

Elastic Load Balancer(ELB)---- {HIGH AVAILABILITY, Health Checks}

---Before Elastic Load Balancer what we do. If your hosting any website, You should have a server for your website and there is IP address for that server and you are attaching IP Address to domain name of your website. Whenever user wants to connect to your website they can simply search the Domain Name of your webiste that will redirect to your server.

---Elastic Load Balancer will map the IP Addresses of all Servers to Domain Name of Website called "CNAME".

---Elastic Load Balancer is also like a software which is installed in server, it may not bare number of requests made by user. it also requires auto scaling.

--- Most Important Pre-Requisite of ELB is should have Two Subnets to create a Load Balancer. Load Balancer is not created in single availability Zone. Suppose you are having 2servers in AZ1 and you wants to create a loadbalancer means it won't create. 

--- Open System Interface Model. It is created by Inetrantional Standards Organization. There will be 7 Layers in this OSI Model. 
--- 1.Application Layer
--- 2.Presentation Layer
--- 3.Session Layer
--- 4.Transport Layer
--- 5.Network Layer
--- 6.DataLink Layer
--- 7.Physical Layer

######################################################################################################################################
Vertual Private Cloud
Tiers:
1 Tier - WEB Tier, APP Tier, DATABASE Tier Altogether in one Place
2 Tier - WEB Tier, APP Tier at one Place
3 Tier - All these THREE are in different places.
######################################################################################################################################

---@@@ HOW TO CREATE ELASTIC_LOAD_BALANCER @@@---
---Search Elastic_LOAD_BALANCER             (OR)
---Search LOAD_BALANCING in left side pane
---First CREATE TARGET_GROUPS
---Give TARGET_GROUP_NAME
---Select TARGET_TYPE
---Select PROTOCOL
---PORT
---Select Desired VPC
---###ADVANCED_SET_HEALTH_CHECK_SETTINGS
---PORT
---Set Healthy_Threshold
---Set UnHealthy_Threshold
---Set Timeout
---Set Interval
---Set Success_codes
@@@TARGET_GROUPS is Created
---Select_Applcation_LoadBalancer
---Give Name of LoadBalancer
---Scheme
---Set IP_Address_Type
---Set Availability_Zones

#####################################################################################################################################

ICANN- INTERNATIONAL CORPORATION FOR ASSIGNING NAME AND NUMBERS: 
Numbers Matalab- IPAddresses
Names Matalab- Domain Names
There are number of Domain Name providers registered under ICANN Called "DOMAIN_REGISTRARS"
E.g: Amazon Registrar
TLD: Top Level Domains e.g: .com, .in, .gov, .org, .edu    www.ajay.in
Domain Registry: This will provide TLD. 
Domain Name Portability:You can change The Backend provider of the Domain Name. Suppose you have taken Domain Name in Route53 and you can port it to other service Providers like Godaddy, azure, GCP {Feasibility}

DNS Providers:

Cloudflare DNS.
Amazon Route 53.
DNS Manager.
GoDaddy Premium DNS.
BloxOne DDI.
NS1.
ClouDNS.
DNSMadeEasy.

Route53----
--- If a website needs a name, Route 53 Registers the name for the website(Domain Name)
--- Route53 helps to connect the browser with the website or web application when the user enter the domain name
--- Route53 checks health of resources by sending automated requests over the inetrnet to a resource
--- Ensures a consistent ability to Route the applications {Highly Reliable}
--- Automatically handles large queries without the user's interaction
--- Easy to sign up, configure DNS settings and provides facts response to queries
--- Pay only for the service used {Cost Effective}
--- Only Authorized users can access the Route53 Like IAM Users {Security}
####################################################################################################################################
VERTUAL PRIVATE CLOUD----
A virtual private cloud (VPC) is a secure, isolated private cloud hosted within a public cloud. VPC customers can run code, store data, host websites, and do anything else they could do in an ordinary private cloud, but the private cloud is hosted remotely by a public cloud provider. (Not all private clouds are hosted in this fashion.) VPCs combine the scalability and convenience of public cloud computing with the data isolation of private cloud computing.

Imagine a public cloud as a crowded restaurant, and a virtual private cloud as a reserved table in that crowded restaurant. Even though the restaurant is full of people, a table with a "Reserved" sign on it can only be accessed by the party who made the reservation. Similarly, a public cloud is crowded with various cloud customers accessing computing resources – but a VPC reserves some of those resources for use by only one customer. 

NAT GATEWAY: How to Downlaod Files, Update the Files, Install the Application in Web Server, App Server and DB Server Private Subnet. Here NAT Gateway Comes into Picture, NAT Gateways are created in Public Subnets and is attached to the route table of Private Subnet. Here unique Feature of NAT Gateway is it permits the other users in using the private servers. No requests will be shared to Private Servers since NAT Gateway not allows it. There is no Two Way Traffic. Only One Way Traffic is possible. Whenever the owner or Admin

INTERNET GATEWAY: Internet Gateway allows Two way connection which helps to connect to internet. Ingress ntework which allows the traffic inside the VPC. Egress Network which allows the traffic to Outside.

NACL(Network Access Control List): NACL Works as Security Firewalls to Subnets. Each subnets will have their own NACLs. You can configure the NACLs to give permissions to the users who can be able to enter into the Subnets.

SECURITY GROUPS: SecurityGroups Works on servers in the network. You can open the port numbers in Security Groups and you can define who can access that port numbers

VPC FLOWLOGS: If you want to know the request sending by you is passing through from which Server to which server. You can simply enable the flow logs bye default it is disabled but you can enable it. It stores the records. 

09/01/2023..
How to create VPC in AWS:
---STEP:1 Search For VPC in Search Bar
---STEP:2 Click on your VPC
---STEP:3 Select Create VPC
---STEP:4 Give some Name for VPC
---STEP:5 Give IPv4 CIDR Block range {10.0.0.0/16}
---STEP:6 Keep Tenancy as a Default One
---STEP:7 Create VPC

Creating Subnets:
---STEP:1 In the left side pane of the VPC DashBoard, U can select Subnets
---STEP:2 Click on create Subnets
---STEP:3 Select Your VPC to attach subnets
---STEP:4 Give name of Subnet
---STEP:5 Select a AZ {Availability Zone}
---STEP:6 Give CIDR Block Range to subnet {10.0.0.0/24}
---STEP:7 Click add another subnet
---STEP:8 Same as previous 4,5,6 Steps.

Creating Route Tables
---STEP:1 Select Routetables in VPC Dashboard 
---STEP:2 Create RouteTable
---STEP:3 Name Router
---STEP:4 Associate Subnet

Create Internetgateway
---STEP:1 Select Internet Gateway in VPC Dashboard
---STEP:2 Create InternetGateway and attach it to VPC
---STEP:3 Go To Private/public Route Table
---STEP:4 Edit Routes
---STEP:5 Add Internet 0.0.0.0 and select target Internetgateway

HOW TO CONNECT PRIVATE SERVER USING PUBLIC SERVER
---STEP:1 LOGIN TO YOUR PUBLIC SERVER
---STEP:2 ADD PEM KEY FILE By using vi editor {vi private.pem}
---STEP:3 chmod 400 private.pem
---STEP:4 ssh -i private.pem private DNS

CREATION OF NAT GATEWAY{Network Address Translation}
---STEP:1 Select NAT GATEWAY from VPC Dashboard
---STEP:2 Choose Create NAT GATEWAY 
---STEP:3 Give name
---STEP:4 Select Public Subnet {Why Not Private Subnet means, If you place NAT Gateway in Private subnet means the internet that is allowed to NAT Gateway leads to others can also enter into private subnet, it leads to data hacking by hackers }
---STEP:5 Select Connectivity Type Public {if company is having on premises servers or their own data cenetrs can select that one}
---STEP:6 ALLOCATE ELASTIC IP {It creates ELASTIC IP}
---STEP:7 Attach NAT GATEWAY to Private Subnet Edit the Route.

AUTO SCALING:
---Horizontal Scaling means it scales the servers horizontally. It means it increase the servers with same configurations like 2core and 4GB Ram for all servers.
---Vertical Scaling means like if you want to improve the server capacity to only one server by eliminating the other previous server
---Drawback of vertical scaling is downtime.
---If the application is in different subnets in different availability zone, you should create separate loadbalancer.
--- DNS Name may also different for that application or website
---Instead of creating different loadbalancers. we can create a loadbalancer for DNS Name

CREATING AUTOSCALING:
---Create VPC
---Create Subnets
---Create Internet Gateway
---Check the RouteTable for each Subnet
---Create LoadBalancer
---Select application loadbalancer
---Give the name of the LoadBalancer
---Select the scheme- 1.InternetFacing 2.Internal {if you want for Private Load balancer}
---Select Default listner e.g: HTTPS
---Select the VPC {Needs to select atleast 2 Availabilty zones}
---Create Security Group, Give name of SG and open the port number 80 fot HTTP
---Configure Routing
---Give the Target Group Name and Target Type {Instance}
---Health Checks {Give the path of application file}
---You can also check the DNS of loadbalancer in DNS Health Checker in the web browser

---cookiecutter
---Launch Normal Instance
---Choose VPC of autoscaling Group
---Select public Subnet
---Open Port Number 80 in Securitty Group
---Create a new key pair
---launch instance
---Login to instance
---Install httpd in the server
---create a directory in /var/www/html e.g: /app
---create index.html file in /app directory
---if you want to start the service of httpd whenever the server is On {chkconfig httpd on}

---Now, Go to Your AWS Console and select your instance, click on actions
--- Select Create image
---Give the image name and save it.

----AutoScaling Launch Configuration
---Name the Launch Configuration
---Select the instance type
---Select the AMI
---Select the Security group and name it
---Open the port number 80
---Select/Create the key pair here.

---Create AutoScaling Group
---Give Autoscaling Name
---In Launch Configuration Template choose Launch Configuration that you have created
---select VPC
---Select subnet
---Attach from existing loadbalancer
---In Health Checks select ELB
---Grace Period {Suppose if your CPU Utilization is reached the threshold value like maximum Utilizatiob the auto scaling wait for somtime to check whether that cpu utilization is optimized or not.}
---Here we need to give graceperiod time
---Group Sizing
---Desired Capacity {When you are launching autoscaling group how many instnaces you want}
---Minimum Capacity
---Max Capacity
---Scaling Policies
---Select Tracking Scaling Policy
---Give the policy Nmae
---Metric type {Average CPU Utilization}
---Give Target Value ()
---Don't select disable scale in to create only a scale out policy

---Now, Try to create Target Group for application2
---Select the protocol number 80 to listen from HTTP
---Select the VPC
---Select the Health Checks and give the path of application2

CLOUDWATCH ALARM----
---Cloudwatch monitors most of the Services in AmazonWebServices
---Some of Monitoring Tools are 1.Nagios, 2.Splunk, 3.Jabix, 4.DataDog
---CPU Utilization
   Memroy Utilization
   Harddisk
STATUS CHECKS: Your EC2 Checks should be - 2/2 {If it is 1/2 you can't able to connect your Server}
---> System status checks
---> Instance status checks
---In ec2 instance select your server
---In that please select Status Checks
---Select Actions
---In That Select status checks Alarms
---Add or Edit Alarams {Create a New Alarams}
---Add Simple Notification Service{SNS}

How To Create Cloud Watch Alarms
---First of all create SNS
---Just follow the link pasted Below
---https://docs.aws.amazon.com/sns/latest/dg/sns-getting-started.html	
---Follow the Simple Steps below to create Cloud Watch alarm.  
---https://docs.aws.amazon.com/sns/latest/dg/sns-getting-started.html#step-receive-delete-message	

Basic Monitoring: It comes by by default
                  Free of Cost
                  Every 5 minutes

Autoscaling--- Scaling Policies

############################################################################################################################           
                                          ----VERSION CONTROL IN SOFTWARE----
############################################################################################################################
---Let's imagine there's a Multinational Company which is having offices and employess all around the globe
---There are number of challenges the company may face like collaboration. there are set of people working in the same project in different regions
---The other challenge may be like storing version the project is not completed with a single version. There may be n number of versions. The problem here is stroing the commits in a single place is big challenege.
---The another challenge is Restoring Previous Versions. It is very important to store the previous version. why because suppose a bit of code was updated in previous code but is having bugs leads to application working issues on that time developer needs to call back the previous code
---Backup: It is also a kind of challenge where developer doesn't have backup disk and it he lost the data means, he can't get back the things that he lost. All efforts may go into vein.
---All these problems can be solved with the help of a "Version Control System".
---The version control system will take care of developers collaboration and also stores the previous versions. Here, Whenever the developer can rollback to previous versions.

---BENEFITS OF VERSION CONTROL: Helps and managing source code and protecting the source code
---Keep Track of all the Modifications made of the code. Like who modified the code, who made changes in the code date, time and all.
---Comparing earlier versions of the code.
---Best Version Control System: GitHub, GitLab, Perforce, Beanstalk, AWS Code, BitBucket, Tortorise.

INTRODUCTION TO GITHUB----
---Git is the most popular version control system.
---First of all let us know about what is push and pull.
---Whenever the Developer commits the code into the repository is called push.
---Whenever the Developer or any other person pull the code from the repository is called pull.
---There are two Different Types of Repos
---CENTRALISED REPO: Let us take an example two developers are working on versions like v1 & v2 which are stored in centralised repo. suppose if you lost the internet connection. You can't pull the code from the repository.
---DISTRIBUTED REPO: Here we take another example like two developers are working on versions like v1 and v2 which is stored in server and also in local repository. The version is stored in his hard drive and even there is no Internet Connection. They can work on their code.

----Alternative Tools for Source code Management tool: "Git, GitLab, Bit Bucket, Mercurial, Subversion" 

GITHUB ACCOUNT CREATION----
---Search GITHUB Account signup in your Web Browser
---Choose always official site.
---Give your Mail ID & Create Password (Username also), Click on Continue Button.
---You need to verify your gmail that you have given and create your account.
---Install Git on your server.
---You can clone the git on your server, just copy the repo web address in git and paste it in your server.
---git clone https://github.com/organisationName/reponame.git
---You can create files and directories from your command line interface itself and push it to the github

GIT COMMANDS----
---git init {This command is used to create a empty repository locally}

---git add . (or) git add filename

---git rm filename {It removes file or directory}

---git commit -m "example messgae"

---git push
---Here it will asks your mail id and passwd{In the place of passwd you need to give github token which can be generated in github}

---> How to Generate the Token settings/DevloperSettings/personalaccesstoken/select repo click on Generate token.

---git checkout branchname {To swtch between braches}

---git pull {To pull the updates from git repository}

---git branch -a {it shows number of branches which are created in remote and Local repository}

---git fetch is the command that tells your local git to retrieve the latest meta-data info from the original (yet doesn't do any file transferring. It's more like just checking to see if there are any changes available). git pull on the other hand does that AND brings (copy) those changes from the remote repository.

---You can pass 2commands in one shot { git add. ; git commit -m "committing new chnages"}

---git log {You can check the commits that you have done so far and you can see who made commits at what time and commit message using.}

---git show Commit_id {it shows you full details of commit}

---git revert Commit_Id {If you want revert back the commit that you have did previously, you can use this command}

---Git Architecture Explanation
****   When you made some changes in Working Directory of Local Repo the Flow is like below
   Do some changes in a file in Working Directory and give Command {GIT STATUS} {Output: file Modified in Red Color} [UN-TRACKED FILE]
   Now Check the status of changes with Command {GIT ADD .} {Output: file Modified in Green Color}
   If You want some files to be untracked, give command {GIT RESTORE --STAGED FILE_NAME}
   Now Give the Command {GIT COMMIT -M "    "} ,It will be in Unmodified stage. Changes of file will move from working Directory to Local Repo
   Now Do {GIT PUSH}, it will push the changed file to Remote Repo
****   When You made Some Changes in Remote Repository and wants to see changes in local do follow below
   Give {GIT PULL} Command
--- CENTRAL REPO: For Example There are 3 developers working on same project and they are developing different features of application and push the code to central repo, there they can integrate the code.   

---> .git folder contains the Dependcies of git, binaries, source code all the things in .git directory
---> IGNORING CONTENT: It will be useful when you don't want to track some specific files then we use a file called (.gitignore)
---> vi .gitignore >> filename >> save the file.
---Some More Commands on GIT BRANCH 
---To Add New Branch: git Branch Branch-name
---To Create and Switch at a Time: git checkout -b branchname
---To Rename a Branch: git branch -m Old New NewName
---To Delete a Branch: git branch -d <branch> (-D force delete) Rule: You Can't delete the current active branch, checkout from it to delete branch.
---Suppose assume you are a developer and you have made changes in particular file. Unfortunately you made a change in unwanted file and now you dont want push it but you want to save it, now do the following:
              git add filename
              git stash
--- If you completed the changes on target file and you want to push the file 
              git stash apply 
--- If you want to get the changes happend in particular branch and you want get the changes in another branch use the command.
              git cherry-pick commitid
---https://www.atlassian.com/git/tutorials/using-branches/git-merge

---> Git Merge Example

     $ git checkout master

     Create a new branch based on master:

     $ git branch feature

     $ git checkout feature

    Adding the footer file:

     $ git add footer.php

    Now, commit the changes:

    $ git commit –m “added footer component”

   After the work is done for adding the footer component, you may merge it into the master branch as follows:

    $ git checkout master

    $ git merge feature

  The above command should merge the feature branch commits into the master branch, so both branches are now at the same level.

What Is Git Rebase?
Rebase is one of two Git utilities designed to integrate changes from one branch onto another. Rebasing is the process of combining or moving a sequence of commits on top of a new base commit. Git rebase is the linear process of merging.

What Does Git Rebase Do?
A Git rebase changes the base of the developer’s branch from one commit to another, so it looks like they have created their branch from a different commit. Internally, Git creates a new commit and applies it to the specified base. However, it's essential for everyone involved to understand that although the branch appears the same, it's made up of entirely new commits. When you perform a Git rebase, you are, in effect, rewriting history.

Here’s how Git rebasing compares to Git merging. Let's say you're a developer who is working on a new feature on a dedicated branch. Then, another development team member updates the main branch with some new commits. The situation looks like this:

Git_Rebase_1.

Eventually, however, the team concludes that the main's new commits are relevant to the feature you are working on. So then, if you want to incorporate the new commits onto your branch, you can either do a merge or a rebase. If you decide to use Git merging, you tack on the new commits to your new branch like this:

Git_Rebase_2.

However, if you use Git rebase, you move your whole feature branch, starting it on the tip of the main branch so that all the new commits are now part of the whole. This action rewrites the project history by making new commits for each of the original branch's commits.

What is Git Rebase, and How Do You Use It?
By John Terra
Last updated on Feb 24, 2023127078

What Is Git Rebase, and How Do You Use It?
Table of Contents

What Is Git Rebase?What Does Git Rebase Do?What Is Git Rebase: Git Rebase UsageGit Rebase Standard vs. Git Rebase InteractiveHow to Git RebaseView More
Developers today face an ever-increasing demand for more applications. Consequently, developers must ensure they have the best tools for the job. The DevOps design methodology has a good collection of tools and resources for the developer, including Git.

Git is an open-source version control system often used for source code management. It features a plethora of commands and functions that make the developer’s job easier. That’s why today we’re here to discuss the Git rebase command.

This article provides a deep dive into rebase in Git. We’ll explore what Git rebase is, what it does, and how to use it. We will also cover other related concepts such as Git rebase branch, Git merge rebase, and Git pull rebase.

So, let's start off with the question, "What is Git rebase?"

Learn from the Best in the Industry!
Caltech PGP Full Stack DevelopmentEXPLORE PROGRAMLearn from the Best in the Industry!
What Is Git Rebase?
Rebase is one of two Git utilities designed to integrate changes from one branch onto another. Rebasing is the process of combining or moving a sequence of commits on top of a new base commit. Git rebase is the linear process of merging.

What Does Git Rebase Do?
A Git rebase changes the base of the developer’s branch from one commit to another, so it looks like they have created their branch from a different commit. Internally, Git creates a new commit and applies it to the specified base. However, it's essential for everyone involved to understand that although the branch appears the same, it's made up of entirely new commits. When you perform a Git rebase, you are, in effect, rewriting history.

Here’s how Git rebasing compares to Git merging. Let's say you're a developer who is working on a new feature on a dedicated branch. Then, another development team member updates the main branch with some new commits. The situation looks like this:

Git_Rebase_1.

Eventually, however, the team concludes that the main's new commits are relevant to the feature you are working on. So then, if you want to incorporate the new commits onto your branch, you can either do a merge or a rebase. If you decide to use Git merging, you tack on the new commits to your new branch like this:

Git_Rebase_2.

However, if you use Git rebase, you move your whole feature branch, starting it on the tip of the main branch so that all the new commits are now part of the whole. This action rewrites the project history by making new commits for each of the original branch's commits. So, this is how the new branch looks:

Git_Rebase_3.

Source

What Is Git Rebase: Git Rebase Usage
Why do people use Git rebase? For one overriding reason: maintaining a linear project history. Let's say for instance that you've been working on a feature branch off the main branch, but the latter has progressed. But you want to get the main branch's latest updates into your feature branch while keeping your branch's history clean, so it looks like you've been working off the updated, latest main branch.

You will benefit from an eventual clean merge of your feature branch back into the main branch, perpetuating a clean history. It's essential to have a clean history, especially when conducting Git operations, to locate and investigate a possible regression introduced into the branch.

To sum it briefly, when you conduct a Git rebase, you’re saying that you want your changes to be based on what other developers have already done.

Basics to Advanced - Learn It All!
Caltech PGP Full Stack DevelopmentEXPLORE PROGRAMBasics to Advanced - Learn It All!
Git Rebase Standard vs. Git Rebase Interactive
There are two different Git rebase modes, standard and interactive. A standard mode Git rebase automatically grabs the commits present in your current working branch and immediately applies them to the head of the passed branch.

On the other hand, Interactive mode lets you change different commits in the process instead of just scooping up everything and tossing it into the passed branch. If you use interactive mode, you can remove, split, or alter existing commits and clean up the history, and we've already touched on why clean histories are essential.

So how do you perform a Git rebase? 

How to Git Rebase
Here’s the syntax for launching a standard Git rebase:

git rebase <base>

And here’s the syntax for launching an interactive Git rebase:

git rebase --interactive <base>

This command opens an editor that lets you enter commands for each commit you want to rebase.

Later, we’ll explore a broader range of rebase commands. But before we do, we must discuss configuration.

---Git Hub Token { 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 } 

-----GIT TASKS:
---1.How to do SSH with Github repository
---2.Find out particular_user commits
---3.Find out febraury 25th commits(Particular Date)
---4.Find out how many commits happend in particular_file (e.g: file1.txt)
---5.In Particular commit How many changes i did?
---6.How many commits i did between particular date to particular date or last 3days.
---7.How to change the commit message.
---8.How to revert changes from staging area.
---9.How to revert the changes after git command.
---10.Assume there are 4 files which is having some similarity in the names and you want to track the files from working directory to local directory. Do....
---11. Difference between git pull and git fetch
---12.How to revert git merge
---13.How to delete a branch and how to revert deleted branch

##################################################################################################
About MAVEN:
---MAVEN is chiefly used for Java-based projects. This tool helps in building the code downloading dependencies. It simplifies the day to day work of Java Developers and helps them in the project.
---Maven is a Build Tool which is Developed by Apache Software Foundation and it is a Open Source Tool.
---Let me take a example, if you are having Java Project we need some third party dependencies. suppose if you are working with MySQLDatabase you need JAR Files{Java Archived Files}, If you are working with Testing(selenium) you need JAR Files and attach to Java Project.
---In Manual Process for those dependencies you will go to different websites and download the dependencies and attach to java project.
---Suppose if you want to improve the version of your project, you need to improve the version of dependencies as well. again you need to remove the existing prject and will again add new versions.
---To Overcome this issues MAVEN Tool comes into the picture.
---Whenever You creating Maven Project by default it creates pom.xml file.
---In pom.xml file it contains the Dependencies and plugins
---When you are having Dependencies on pom.xml it will automatically download third party applications for the project.
---Plugins another type of entires which is in the pom.xml, it is basically having configuration stuff like compiling the project, running the project and some other type of configurations which you specify inside the plugins.
---This Two Plugins and Dependencies Will control entire project.
---Maven also provides Project Structure
---It can generate Project related reports and Documentation.
---Maven will do the packaging of the project.
----How Maven Project Works Internally? Like how it downloads the dependencies from different websites and how it is attaching to the project.
---Maven is based on "Project Object MOdel" and focuses of simplication and standardization of the Building Process.
---In the pom.xml {Project Object Model}file we have <dependencies> tag will be there. Here we will specify the Dependencies for entire project.
---As soon as you are creating maven project, it will create maven local repository internally ".M2 directory". As soons as you are giving dependencies in the tag, It will download the JAR files of dependencies into local repository. Actually where it download the dependencies, it downloads from the Remote Maven Repository.
---Website Name: mvnrepository.com
---Remote Maven Repository conatins the all JAR Files, Whenever you are specifying the dependencies in the pom.xml file, it will check the remote maven repository whether the specified version of that dependencies are there or not.
---To get all the JAR Files we should have internet connection.
---Once all the Dependencies downloaded into the local repository which is in your local system in that case you don't need any internet connection.
---If you want to update version of the dependencies, you need to specify in the pom.xml.  
---What is the use of Plugins: Project Related Configurations
---Suppose if you want to compile a project, then we need to add maven compiler plugin.
---If you want to run the project, then we need to add surefile plugin.
---Compiler: Compile refers to the act of converting programs written in high level programming language, which is understandable and written by humans, into a low level binary language understood only by the computer.

@@@@_Pre-Requisites to install Maven in your Server:
--- Server Configuration t2.micro, AMI: Any
--- Java Application(Oracle Corporation)
    (sudo amazon-linux-extras install java-openjdk11) ---- (yum install java-11-openjdk)
--- Git (In ubuntu git is installed by default)
--- Install maven.
--- Open WebBrowser and Search for " https://downloads.apache.org/maven/maven-3/ "
--- Select Binaries Here
--- Copy the link here
--- Download the tar file and Un-tar file
--- Maven Repo is installed
--- yum install maven -y
--- mvn archetype:generate (It Downloads Maven Related Repositories, Documents, plugins will be downloaded over internet)
---What is JDK:- Java Development Kit
---What is JRE:- Java RunTime Environment
---Difference JDK & JRE: In JDK Will get Compiler by default, but not in JRE.
---JavaC - Java Compiler.
###################################################################################################################################
---MAVEN LIFE CYCLE::: (LifeCycle---Phases---Plugins---Goal)
--- (Build/Default, Clean, SiteLifecycle)
--- Build Life Cycle Consists of a sequence of build phases and each build phase consists of a sequence of goals.
---Each goal is resposnible for a particular task.
---When a phase is run all the goals related to that phase and its plugins are also compiled.
---Let us take example of Build Life Cycle. Here, different phases will be there they are validate, compile, test, packaging and install.
--- For Compile you need to give " compiler plugin " in POM.xml
--- For Test you need to give " SureFire Plugin " in POM.xml
--- For package you need to give JAR Plugin, War Plugin, Ear Plugin.
--- JAR Means JAVA ARCHIVED FILES
--- WAR Means WEB ARCHIVED FILES
--- EAR Means ENTERPRISE ARCHIVED FILES
--- Maven CleanLifeCycle (PreClean---Clean---PostClean)
--- Maven SiteLifeCycle (PreSite---Site---PostSite)

---> MAVEN COMMANDS:
* mvn compile
* mvn test
* mvn package
* mvn install
* mvn deploy
* mvn clean install --- It will do all the build phases at a single shot
* mvn clean install -D maven.test.skip=true --- It Skips the test

###################################################################################################################################
                                                         JENKINS
###################################################################################################################################
Jenkins:
========
--> Jenkins is developed on Java and it is a CI/CD (Continuous Integration/Continuous Delivery) tool.
--> Repeated tasks will be done by Jenkins.
--> Default port for Jenkins is 8080.

Advantages of Jenkins:
======================
--> It is an open source tool with great community support.
--> It is easy to install.
--> It has 1000+ plugins to ease your work. If a plugin does not exist, you can code it and share with the community.
--> It is built with Java and hence, it is portable to all the major platforms

Terminologies:
==============
Jenkins Home Directory(/var/lib/jenkins):
-----------------------------------------
    -->This is default home directory for our jenkins.
       Where my all jenkins configuration such as jobs, users, plugins information will be saved.

Build Directory(/var/lib/jenkins/jobs/<jobname>):
-------------------------------------------------
    --> Where build information will be saved (Always build information in master).

Workspace/Root Directory(/var/lib/jenkins/workspace/<jobname> if it is a master):
----------------------------------------------------------------------------------
    --> Where build steps will be triggered and SCM steps also executed here. This can be master or slave.
 
--> We can remove the old builds in jenkins console.
    ->Under Configure -> Discard Old Builds

###########################################################################################################################
 Jenkins job steps:
1) job name/description/where to run
2) where is ur code. (SCM)
3) when do u want to run this job (build trigger)
4) what build steps  to run (build steps)
> build an app
> sonar checks
> nexus upload
> deploy into dev
> notification
###############################################################################################################################

Jenkins supports different types of build jobs. Some of them are:
 
Freestyle Project
-----------------
Freestyle build jobs are general-purpose build jobs, which provides a maximum of flexibility. It can be combined in SCM with any build system and this can be even used for something other than software build. It is the central feature of Jenkins.
 
Maven Project
-------------
Build a maven project. Jenkins takes advantage of your POM files and drastically reduces the configuration
 
Pipeline
--------
Here we can easily identify which step is failing no need to check in console output. Everything will be in script and will get the individual stages of output which helps us to understand where it is failing. Pipeline will be in Groovy Script.
 
Multibranch Pipeline

It will have more functionalities when compare with pipeline such as auto creation/deletion of jobs.

################################################################################################################################

I am running my job in slave machine
 
Feature job >> daily builds will be happend when developer commited the code
 
When developer commited new code to repo my jenkins daily build will be triggered
 
git checkout into intermediate or dev server >> git should be installed on slave
 
application build >> maven is required on slave machine.
 
sonar checks >> sonar can be installed any where and sonar info should be added to jenkins master
 
nexus >> nexus  can be installed any where and nexus info should be added to master.
 
web deployment into tomcat >> tomcat can be installed on same machine or different machine and add the info to jenkins master.


Jenkins build Steps:
====================
git checkout
maven build
sonar checks
nexus upload
deploy war file to tomact server

#############################################################################################

How to add Slave:
--> Need to add shell /bin/bash for jenkins user in /etc/passwd.(master)
--> Login to master with jenkins user (su - jenkins).
--> In slave set the password for root user and enable the password authentication to "Yes" in /etc/ssh/sshd_config and restart the sshd service.
    If it is a normal user then we have to add it in sudo as well.
--> Do the ssh to slave from Master with root.
--> Install the java in slave (must and should java required in slave)
    (https://www.tecmint.com/install-apache-maven-on-centos-7/)
--> Finally add slave into jenkins from console.

#################################################################################################
sudo update-alternatives --config java
--> How to reset user password in jenkins
    -> Manage Jenkins -> manage users >> Click on user -> Go to Configure -> Password ( give new password and save)
 
--> If we want to give any previliges to any user.
    -> Manage Jenkins -> Configure Global Security -> Project based matrix -> Add user or Group -> Give Permissions -> save
--> If we want to stop/disable the jobs/project.
    -> Configure -> Disable the project
--> If admin forgot password
    i) Go to putty(server) and jenkins home directory
    ii) #cd /var/lib/jenkins
    iii) #vi config.xml
        <usesecurity>true</usesecurity> (By default it will be true)
        change from true to false and then restart the service
    iv) This time GUI will not ask the credentials to login, it will login directly
--> Once we login if we want to revert the changes then,
    i) manage jenkins -> configure global security -> enable the security -> jenkins own user database -> save
    ii) Go to manage users -> select the user and reset the password under configure
    iii) If we change password in GUI and enabled then in the config.xml file it will become true.
 
###############################################################################################################

Plugins:
========
--> We can integrate any tool to jenkins with plugins.
 
--> Installed plugins will be available under below,
    manage jenkins -> manage plugins -> installed
 
--> Available plugins will be under below,
    manage jenkins -> manage plugins -> available
 
--> If we want to install any plugin then go to available tab and search with plugin and click on install
    ex: maven
######################################################################################################
Maven:
======
--> It is a developer tool/build tool for java project.
--> As a DevOps admin we just need to install the maven.
--> The path for maven is: /etc/profile.d
                vi maven.sh
--> The below are the life cycles of maven(means commands), Developers will give the life cycles to us.
    validate, compile, package, test, verify, install, deploy
##########################################################################################################

Installation of Sonarqube and Nexus:
====================================
--> Sonarqube is to verify the code quality interms of bugs,vulnerabilities.
 
--> As a devops admin we just need to install, configure and integrate to jenkins.
 
--> Sonarqube will generate the reports.
 
--> Nexus is just to store the data with security.
 
Port Numbers:
=============
Tomcat    -- 8080
Nexus    -- 8081
Sonarqube -- 9000
Jenkins    -- 8080
#####################################################################################################

---> Send automated email to the dev team if build fails
a.	In Order to send and receive emails we need to configure email servers in jenkins.
Companies has their own email servers, we have to use those servers to trigger   
Emails, in our example we do not have our own server, so we are going to use
Gmails SMTP server.
b.Configure gmail SMTP server in jenkins
i.	jenkins → Manage Jenkins → Configure System
ii.	Under E-mail Notification
1.	SMTP server  → smtp.gmail.com
2.	Select Use SMTP Authentication
3.	Put your gmail id
4.	Put your gmail password
5.	Use SSL select
6.	SMTP port  465  
7.	Save the configuration

c.	Configure jenkins job to trigger email if build fails
i.	Open your job → Configure → Post Build Actions → Add post-build action → email notification
ii.	Under Recipients put teams mail id and save the configuration

################################################################################################################################
Nexus config:
 
=============
/usr/local/src/apache-maven/conf/settings.xml
## add below in </servers> componenet
    <server>
    <id>deployment</id>
    <username>admin</username>
    <password>admin123</password>
    </server>

/usr/local/src/apache-maven/bin/mvn package
 
/usr/local/src/apache-maven/bin/mvn sonar:sonar -Dv=${BUILD_NUMBER} deploy
 
<distributionManagement>
   <repository>
   <id>deployment</id>
   <name>Internal Releases</name>
   <url>http://3.8.82.180:8081/repository/maven-releases/</url>
   </repository>
   <snapshotRepository>
   <id>deployment</id>
   <name>Internal Snapshot Releases</name>
   <url>http://3.8.82.180:8081/repository/maven-snapshots/</url>
   </snapshotRepository>
</distributionManagement>


Pipeline syntax:
pipeline{
agent { ‘label labelname’ }
stages{
stage(scm){
steps{
}
                    }
stage(sonar){
steps{
         }
                       }
            }
              }


---> Pipeline example:
pipeline{
     agent { label 'slave' }
     stages{
       stage(scm) {
           steps{
checkout([$class: 'GitSCM', 
    branches: [[name: '*/feature']], 
    userRemoteConfigs: [[credentialsId: '4a1288aa-6391-4d75-9387-f37f8a43bd1d ', url: 'https://github.com/devopsamar/mavenrepo.git']]
])
}
       } 
       stage(package){
       steps{
          sh '/usr/local/src/apache-maven/bin/mvn package' 
       }    
       }
       stage(tomcat){
           steps{
          sh 'ssh 18.188.185.28 systemctlfff stop tomcat'
sh 'scp /root/workspace/My_First_Job/target/studentapp-2.5-SNAPSHOT.war 18.188.185.28:/var/lib/tomcat/webapps'
sh 'ssh 18.188.185.28 systemctl start tomcat'
       }
     }
     }
         }


#################################################################################################################################

*******Difference Between POLL SCM & BUILD PERIODICALLY************
"Poll SCM" polls the SCM periodically for checking if any changes/ new commits were made and shall build the project if any new commits were pushed since the last build, whereas the "build periodically"  shall build the project periodically irrespective to whether or not any changes were made.

*******Difference Between POLL SCM & GitHub WebHooks************
POLL SCM checks Source Code Management Tool periodically for if any changes/new commits were made and shall build the project if any any new commits were pushed since the last build. GitHub WebHooks will poll source code management automatically whenever the changes committed in SCM without concerning time and date. 

##################################################################################################################################

what is webhook? diff between webhook and scm

webhook: webhook is whenever we do any changes in the github repository then the job in jenkins where that github repository is used gets triggered automatically


login to github> go to your repository ex: sample-satwick/sample_application > go to settings > click on webhook> create webhook> 

Payload URL
http://65.0.105.99:8080/github-webhook/   		(this url is where jenkins is running and add github-webhook/ at the end)

Content type
application/json

save it
	
and in the jenkins job add this github repository and
in build triggers select "GitHub hook trigger for GITScm polling"  save the job 
now when u do any changes like add new file in github repository of sample_application then automatically in jenkins the job gets triggered.


diff between webhook and scm:
------------------------------

While polling and webhooks both accomplish the same task, webhooks are far more efficient.
When using polling, the frequency of your polls limits how up-to-date your event data is. For example, if your polling frequency is every 12 hours, 
the events returned by any poll could have happened any time in the past 12 hours. This means that any time an event occurs in the endpoint, your app 
will be out-of-date until the next poll.

With webhooks, this problem is eliminated. Since events are posted immediately to your monitored URL, your apps will automatically update themselves with 
the new data almost instantly.


#####################################################################################################################################
                                                              TOMCAT
#####################################################################################################################################
---> We have two different types of websites:
1.Static Website: This website contains same type of info which can be visible to all the users, It Responds fastly comparing to Dynamic type application website and maintaining this static website is very easy comparing to Dynamic website.

2.Dynamic Website: This Website changes dynamically user to user and it responds little bit late comparing to Static Website.

Tomcat: Tomcat is application server. It is java based application. It provides a platform to the java application.

Difference Between Web server and Application server

          80                                                                            8080
        HTTPD                                                                          TOMCAT
 /etc/httpd/conf/httpd.confg                                                     /opt/apache-tomcat-xx/conf
    /var/www/html                                                               /opt/apache-tomcat-xx/webapps
     .html/.php                                                                        .java

PREREQUISITE TO INSTALL TOMCAT:
1.Java
2.Tomcat

----Download Tomcat Tar File
----Untar the downloaded File
----https://www.digitalocean.com/community/tutorials/how-to-install-apache-tomcat-7-on-centos-7-via-yum   

##########################################Troubleshooting Tomcat Issues:#################################################

---> Before accessing the tomcat application in web browser make sure that Application is started or not in the terminal.
* systemctl status tomcat

---> Make sure that you should open port number 8080 in security Groups

---> 403 Access Denied ----> You are not authorized to view this page. 
If you are getting this error, you need to Make some changes in "Conf/tomcat-users.xml", add the below line on that file at the bottom of the file:
* vi /opt/tomcat/conf/tomcat-users.xml
* <user username="tomcat" password="password" roles="manager-gui"/>

---> Even After creating role in Conf/tomcat-users.xml in this file and you are facing the same issue means then do,
* vi /opt/tomcat/webapps/manager/META-INF/context.xml

In this file find the below lines

<Valve className="org.apache.catalina.valves.RemoteAddrValve"
         allow="127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1" />
  <Manager sessionAttributeValueClassNameFilter="java\.lang\.(?:Boolean|Integer|Long|Number|String)|org\.apache\.catalina\.filters\.CsrfPreventionFilter\$LruCache(?:\$1)?|java\.util\.(?:Linked)?HashMap"/>


#comment those lines with "<!--" starting of the line and give  "-->" at the end of line.

---> After Commenting those lines, it looks like below:

<!--  <Valve className="org.apache.catalina.valves.RemoteAddrValve"
         allow="127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1" />
  <Manager sessionAttributeValueClassNameFilter="java\.lang\.(?:Boolean|Integer|Long|Number|String)|org\.apache\.catalina\.filters\.CsrfPreventionFilter\$LruCache(?:\$1)?|java\.util\.(?:Linked)?HashMap"/>  --> 

Now, restart tomcat application and check with webbrowser by accessing it.
* systemctl restart tomcat.

###################################################################################################################################
                                                            SONARQUBE
###################################################################################################################################
IN DevOps Developing the code is essential one
Anyone needs to check the code written by Developer
Is it bug free?
Is it secure?
No Duplications? (We can use same type of codes at different places, checking whether they have used it or not)
Tested Properly?
Complex Code? (If developers writes complex code others may feel typical to check the bugs and errors in code)
Easy to Integrate with others Code?
----------------------------------------

* For this Purpose we need to check the code. Who Will check the code?
-> The person who is having coding knoweldge otherthan original developer will check the code. This is called Peer Review.
-> This is manual process and Time consuming process and productivity also decreases.
-> In this case we should have automation tool to run these activites.

#STATIC_CODE_ANALYSIS: This will check what are the problems that are affecting to the code and it will give Review Reports and which allow the developer to check what are the problems in the code and also helps the developer where to develop the code.

Static_Code_Analysis Tools: SonarQube, Coverity, raxis, VERACODE, CodeScene.

### SONARQUBE: This tool not only used for Static Code Analysis and also it is used for Quality Analysis("Quality Management Tool").
- Code Analysis
- Test Reports
- Code Coverage
It shows Reports in Graphical User Interface(GUI)

Components Of SonarQube
--> Sonarqube Server
* Rules: Rules are nothing but conditions, To check bugs, Security Vulnerabilites, Code Complexity are checked. To test all these things we give set of rules. By default you will get rules when you install Sonarqube into the server.
* Database: Rules are Implied on particular code, it will generate the report. That reports are stored in Database.
* WebInterface: It is used to view the data which is stored in Database
* Elastic Search: It is used to search for a Particular report stored in database.

---> SonarScanner
* It is agent of sonarqube
* It is used to run on the source code on the developer system.
* SonarQube supports 27 Programming languages.
* It generates Reports and send it to SonarQube Server.


Sonarqube available in 2 editions
Commercial:
ABAP, C-Family (C, C++, and Objective-C), COBOL, PL/SQL, Visual Basic,
Natural, VB.Net, RPG, Swift..
Open Source:
Java, Java Script, C#, Web(HTML, JSP, JSF, ..) XML, Python, Groovy,
PHP, Puppet, Lua, Groovy, FxCop, Flex, Erlang ...
____________________________________________________________________________________________________________________________________
SonarQube installation in Linux:
Hardware Requirements for SonarQube
---------------------------------------------------------------------------------------------------------------
* The SonarQube server requires at least 2GB of RAM to run efficiently.

---> https://www.fosstechnix.com/how-to-install-sonarqube-on-ubuntu-22-04-lts/
---> SONARTOKEN: f938d86c2cdba8fe73953ca8a8ab739e0642f03c
----> GMail Generated Token: rbghuiaeviasqpxk

* Login as a root user.

Note: MySQL Support for SonarQube is depricated. Increase the vm.max_map_count kernal ,file discriptor and ulimit for current session at runtime.

Give the Below Commands

* sysctl -w vm.max_map_count=524288
* sysctl -w fs.file-max=131072
* ulimit -n 131072
* ulimit -u 8192

To Increase the vm.max_map_count kernal ,file discriptor and ulimit permanently . Open the below config file and Insert the below value as shown below,

sudo nano /etc/security/limits.conf

sonarqube   -   nofile   65536
sonarqube   -   nproc    4096

Before installing, Lets update and upgrade System Packages
* sudo apt-get update
* sudo apt-get upgrade

(Upgrades go from version to version, for example, version 2019 to version 2020. Updates are small changes within the same version, for example, version 2019.1. 0 to 2019.2.)

Install wget and unzip package
* sudo apt-get install wget unzip -y

Step #1: Install OpenJDK
sudo apt-get install openjdk-11-jdk -y
sudo apt-get install openjdk-11-jre -y

SET Default JDK
To set default JDK or switch to OpenJDK enter below command,
* sudo update-alternatives --config java

Check JAVA Version:
* java -version

Step #2: Install and Setup PostgreSQL 10 Database For SonarQube
* sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main" >> /etc/apt/sources.list.d/pgdg.list'

*  wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add -

Install the PostgreSQL database Server by using following command,
* sudo apt-get -y install postgresql postgresql-contrib

Start PostgreSQL Database server
* sudo systemctl start postgresql

Enable it to start automatically at boot time.
* sudo systemctl enable postgresql

Change the password for the default PostgreSQL user.
* sudo passwd postgres

Switch to the postgres user.
* su - postgres

Create a new user by typing:
* createuser sonar

Switch to the PostgreSQL shell.
* psql

Set a password for the newly created user for SonarQube database.
* ALTER USER sonar WITH ENCRYPTED password 'sonar';

Create a new database for PostgreSQL database by running:
* CREATE DATABASE sonarqube OWNER sonar;

grant all privileges to sonar user on sonarqube Database.
* grant all privileges on DATABASE sonarqube to sonar;

Exit from the psql shell:
* \q

Switch back to the sudo user by running the exit command.
* exit

Step #3: How to Install SonarQube on Ubuntu 22.04 LTS

Download sonaqube installer files archive To download latest version of visit SonarQube

*  cd /tmp
* sudo wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.9.9.56886.zip

Unzip the archeve setup to /opt directory
* sudo unzip sonarqube-8.9.9.56886.zip -d /opt

Move extracted setup to /opt/sonarqube directory
* sudo mv /opt/sonarqube-8.9.9.56886 /opt/sonarqube

Step #4:Configure SonarQube on Ubuntu 22.04 LTS
1. Create Group and User:
Create a group as sonar
* sudo groupadd sonar

Now add the user with directory access
* sudo useradd -c "user to run SonarQube" -d /opt/sonarqube -g sonar sonar 
* sudo chown sonar:sonar /opt/sonarqube -R

Open the SonarQube configuration file using your favorite text editor
* sudo nano /opt/sonarqube/conf/sonar.properties

Uncomment and Type the PostgreSQL Database username and password which we have created in above steps and add the postgres connection string.

#sonar.jdbc.username=sonar
#sonar.jdbc.password=sonar
sonar.jdbc.url=jdbc:postgresql://localhost:5432/sonarqube
Uncomment and Type the PostgreSQL Database username and password which we have created in above steps and add the postgres connection string.

Edit the sonar script file and set RUN_AS_USER
* sudo nano /opt/sonarqube/bin/linux-x86-64/sonar.sh

2. Start SonarQube:
Now to start SonarQube we need to do following: Switch to sonar user
* sudo su sonar
Move to the script directory
* cd /opt/sonarqube/bin/linux-x86-64/
Run the script to start SonarQube'
* ./sonar.sh start

3. Check SonarQube Running Status:
To check if sonaqube is running enter below command
* ./sonar.sh status


How to view sonar console?
* before accessing the sonar tool, we should add 9000 port to instance security group
* copy your instance Ip address and paste it in ipaddress
ex: 139.59.34.76:9000
* it will ask you for username and password
 userid admin
 password admin

* it will redirect you to password change page. update your password

TroubleShooting Errors:
--------------------
sonar service is not starting?
a)make sure you need to change the ownership and group to /opt/sonarqube/ directory for
sonar user.
b)make sure you are trying to start sonar service with sonar user.
c)check java is installed or not using java -version command.
d)delete /temp file under sonarqube directory and try to start sonar
Unable to access SonarQube server URL in browser?
a)make sure port 9000 is opened in security groups - AWS ec2 instance.
de135dcbfcf7c37ddb3b1b6f718d323f4c5d0fdc
___________________________________________________________________________________________________________________________________
Create SonarQube server as a sonar service
--------------------------------------------------------
Running SonarQube as a Service on Linux with SystemD

On a Unix system using SystemD, you can install SonarQube as a service. You
cannot run SonarQube as root in unix systems. Ideally, you will have created a new
account dedicated to the purpose of running SonarQube. Let's suppose:
• The user used to start the service is sonarqube
• The group used to start the service is sonarqube
• The Java Virtual Machine is installed in /opt/java/
• SonarQube has been unzipped into /opt/sonarqube/
• create a file /etc/systemd/system/sonar.service using vi editor
• cmd: vi /etc/systemd/system/sonar.service

paste the below code into sonar.service file
[Unit]
Description=SonarQube service
After=syslog.target network.target
[Service]
Type=forking
ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start
ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop
LimitNOFILE=65536
LimitNPROC=4096
User=sonar
Group=sonar
Restart=on-failure
[Install]
WantedBy=multi-user.target

• Enable the sonar service
sudo systemctl enable sonar
• Start the sonar service
sudo systemctl start sonar
• Check the status of the sonar service
sudo systemctl status sonar

Note: For ubuntu Document please follow the below link
https://www.how2shout.com/linux/install-sonarqube-on-ubuntu-20-04-18-04-server/
____________________________________________________________________________________________________________________________________
t2.small instance.

Install Nexus on Ubuntu:
apt-get update
apt-get install unzip
apt install openjdk-8-jre-headless -y
cd /opt
wget http://download.sonatype.com/nexus/3/nexus-3.22.0-02-unix.tar.gz
chmod 777 nexus-latest-bundle.zip (#Use the correct version as per downloaded.)
unzip nexus-latest-bundle.zip
sudo adduser nexus
chown -R nexus:nexus nexus-3.22.0-02/
chown -R nexus:nexus sonatype-work/
rm -rf nexus-latest-bundle.zip
cd nexus-3.22.0-02/bin
RUN_AS_USER=nexus ./nexus start
http://65.2.188.153:8081/nexus/#welcome

______________________________________________________________________________________________________________________________________

######################################################################################################################################
                                                            DOCKER
######################################################################################################################################

MONOLITHIC: We can take example on this like different types of services available in a single application is called Monolithic. E.g: Phonepe is the example for that we have different types of services available in phonepe like banking, movie tickets, Train Tickets, Muncipal Tax payments etc., if you want to update the feature of one service you need stop all services for update. Here, Entire application is deployed into one server.

MICRO SERVICES: In Micro Services Applications are stored in Different servers and if you want to update the feature of one service you don't need to stop entire application. you can update the server where service is stored. The cost of maintaining the servers is hign comparatively to MONOLITHIC.

Here, Docker comes into the picture. We can overcome this issue with the help of Docker.

DOCKER: Docker is the "containerzation Tool", Conatiner is logical vertual machine which is created within in the Single Server. Each container is isolated from other container. The main purpose of using docker is to manage the cost and time. It is advanced than vertualization.

---> It is an Open-source Centralized Platform designed to create, deploy, and run applications.
---> Docker is written on go language.
---> Docker uses container on host OS to run applications.
---> We can install docker on any OS but docker engine natively runs on linux distribution.
---> Docker performs OS level vertualization also known as Containerization.
---> It was intially release in march 2013 and developed by solomonhykes and sebastian pahl.


What is difference between Virtualization and Containerization?

* Virtualization advantages and Dis-advantages:

Adv:
a)Multiple OS in Same Machine
b)Easy Maintenance and Recovery
c)Lower total cost of Ownership

Dis-adv:
a)Multiple Vm's lead to unstable performance
b)Hypervisors are not as efficient as Host OS
c)Long Boot-Up-Process (Approx. 1 minute)

---> Containerization: Containerization is just virtualization at the Operating system level:

Advantages over Virutalization:
a)Containerization on same OS kernel are lighter and smaller
b)Better resource utilization compared to VM's
c)Short boot up process (1/20th of a second)

---> Docker Daemon: Daemon run on host machine. Daemons create and manage Docker objects: Images, Containers, Networks, Volumes, Data, etc. The user does not directly interact with the Daemon, but instead through the Docker client.

---> Docker Client: Primary user interface to Docker. It accepts commands from the user and communicates back and forth with a Docker daemon.

---> Docker Images: Images are used to create Docker containers. Docker provides a simple way to build new images or update existing images. Docker images are the build component of Docker.
 
---> Docker Registries: Registries store images. These are public or private stores from which you upload/download images. This can be done on Docker Hub, which is Docker's version of Git Hub. Docker registries are the distribution component of Docker.

---> Docker Containers: Containers are created from Docker images. They hold everything that is needed for an application to run. Each container is an isolated and secure application platform. Docker containers are the run component of Docker.

---> How to create Dockerhub Account?
Search www.Dockerhub.io in Google
Select signup option
Enter Your Name, Mail ID, Password and signup.

---> How to install Docker in ec2-Instance???
Step 1 — Installing Docker

First, update your existing list of packages
* sudo apt update

Next, install a few prerequisite packages which let apt use packages over HTTPS
* sudo apt install apt-transport-https ca-certificates curl software-properties-common

Then add the GPG key for the official Docker repository to your system
* curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

Add the Docker repository to APT sources
* sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable"
This will also update our package database with the Docker packages from the newly added repo

Make sure you are about to install from the Docker repo instead of the default Ubuntu repo
* apt-cache policy docker-ce

Finally, install Docker
* sudo apt install docker-ce

Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it’s running
* sudo systemctl status docker


Docker COMMANDS:
* If you want to check images in your Local Machine.
 $ docker images

* If you want to pull Image from Docker Hub
 $ docker pull Image_Name

* If you want to create container
 $ docker run -dit image_name

* If You want to check running containers
 $ docker ps

* If you want to start the Container
 $ docker start container_id

* If you want to stop the container
 $ docker stop  container_id

* If you want to check the total number of containers Including Running & Exited
 $ docker ps -a

* If you want to delete the conatiners
 $ docker rm container_id

* If you want to logged into the container
 $ docker exec -it container_id /bin/bash

---> If you want to install any application in your container you need to update your system
 * if it is ubuntu
  $ apt update
 * If it is Centos/amazon-linux/Redhat
  $ yum update
 
---> If you want to create image and push it into DockerHub, Do the following
* Create Image and container and login to that container and make some configurations and come out of it.
 $ docker commit conatiner_id image_name
* Now, login to docker giving command below
 $ docker login

username:
password:

* Now, give push the image into the DockerHub Repository.
  $ docker push Image_name

---> How the end users will connect to the web application which is deployed in container of docker host??
docker run -dit -p 80:80 imagename

---> Dockerfile: 
* It is basically a text file which contains some set of instructions.
* Automation of docker image creation.
* Always D is capital letter on Dockerfile
* And start the components also be capital letter.

---> Docker File Components:

* FROM: for base image this command must be on top of the file.
* RUN: to execute the command, it will create a layer in file.
* COPY: copy files from local system
* ADD: it can download files from internet and also we can extract file at dockerimage side.
* EXPOSE: to expose ports such as 8080 for tomcat and port 80 for nginx etc.
* WORKDIR: to set working directory for the container.
* ENV: environment variables.



$ docker run -it --name container_name img_name

######################################################################################################################################
Amazon Web Services Global Level Infrastructure----
                                REGIONS
                           AVAILABILITY ZONES
                              DATA CENTERS 
CATEGORISED INTO 4
1.North America
2.South America
3.Middle East {Africa, Europe}
4.Asia Pacific{Asia, Australia}
Suppose let us take ap-south-1, How they named this is in asia pacific Region and being mumbai is south most region 
5. Distance between the Data Centers is 100-150km. Bcoz suppose power supply issues, Security Issues, Cooling System, Natural Disasters(Prakruthi Vaiparityam). The data is stored in particular data center may lost.


What are smoke tests in it?
Smoke tests are a minimum set of tests run on each build. Smoke testing is a process where the software build is deployed to a quality assurance environment and is verified to ensure the stability of the application. Smoke Testing is also known as Confidence Testing or Build Verification Testing.
 
566083311859 --Account ID

